# Decision Analyzer Configuration

# LLM Configuration (Ollama)
# For Docker containers, use host.docker.internal to access host machine services
# For local development, use localhost
LLAMA_CPP_URL=http://localhost:11434

# Optional: Secondary Ollama server for parallel processing (persona generation)
# If not set, all requests will use the primary LLAMA_CPP_URL
# LLAMA_CPP_URL_1=http://localhost:11435

# Optional: Dedicated Ollama server for embedding requests
# If not set, embeddings will use the primary LLAMA_CPP_URL
# LLAMA_CPP_URL_EMBEDDING=http://localhost:11436

LLAMA_CPP_TIMEOUT=300

# LightRAG Configuration
# For Docker containers, use host.docker.internal to access host machine services
# For local development, use localhost
LIGHTRAG_URL=http://localhost:9621
LIGHTRAG_TIMEOUT=60

# Application Configuration
LOG_LEVEL=INFO
LOG_FORMAT=json
DEBUG=false

# LAN Discovery Configuration
# Enable LAN discovery to allow access from other machines on the network
# When enabled, the backend will return the HOST_IP in the API config endpoint
# and CORS will allow all origins
ENABLE_LAN_DISCOVERY=false

# Host IP address for LAN discovery (e.g., 192.168.0.53)
# This should be the IP address of the machine running the backend
# Used when ENABLE_LAN_DISCOVERY=true
# HOST_IP=192.168.0.53

# Frontend API URL (optional override for development)
# If not set, defaults to http://localhost:8000
# When LAN discovery is enabled, this will be dynamically fetched from the backend
# NEXT_PUBLIC_API_URL=http://localhost:8000
