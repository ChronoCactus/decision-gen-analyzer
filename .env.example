# Decision Analyzer Configuration

# ============================================================================
# LLM Configuration - LangChain-based (NEW)
# ============================================================================
# The system now supports multiple LLM providers via LangChain
# Choose ONE of the configurations below or use the default Ollama setup

# DEFAULT: Ollama (Local) - Uses ChatOllama for full parameter support
LLM_PROVIDER=ollama
LLM_BASE_URL=http://localhost:11434
LLM_MODEL=gpt-oss:20b
LLM_TEMPERATURE=0.7
LLM_TIMEOUT=300

# Ollama-specific parameters (only used when LLM_PROVIDER=ollama)
OLLAMA_NUM_CTX=64000        # Context window size (CRITICAL for ADR generation)
# OLLAMA_NUM_PREDICT=2000   # Max tokens to generate (optional)

# Optional: Secondary backend for parallel processing (50% faster persona generation)
# LLM_BASE_URL_1=http://localhost:11435

# Optional: Dedicated embedding backend
# LLM_EMBEDDING_BASE_URL=http://localhost:11434
# LLM_EMBEDDING_MODEL=nomic-embed-text

# OPTION: OpenRouter (Cloud - 200+ models)
# Get API key from: https://openrouter.ai/
# OPENROUTER_API_KEY=sk-or-v1-your-key-here
# LLM_PROVIDER=openrouter
# LLM_BASE_URL=https://openrouter.ai/api/v1
# LLM_MODEL=anthropic/claude-3-5-sonnet
# Popular models: anthropic/claude-3-5-sonnet, openai/gpt-4-turbo, meta-llama/llama-3-70b-instruct

# OPTION: OpenAI (Cloud)
# OPENAI_API_KEY=sk-your-key-here
# LLM_PROVIDER=openai
# LLM_BASE_URL=https://api.openai.com/v1
# LLM_MODEL=gpt-4

# OPTION: vLLM (Local high-performance)
# LLM_PROVIDER=vllm
# LLM_BASE_URL=http://localhost:8000/v1
# LLM_MODEL=meta-llama/Llama-2-70b-hf

# See docs/QUICK_REFERENCE_LLM_PROVIDERS.md for more provider options

# ============================================================================
# LightRAG Configuration
# ============================================================================
# The Decision Analyzer includes an optional bundled LightRAG service
# 
# To use the bundled LightRAG service:
#   docker compose --profile lightrag up
#   LIGHTRAG_URL should be: http://lightrag:9621 (when running in Docker)
# 
# To use an external LightRAG service:
#   docker compose up (without --profile lightrag)
#   LIGHTRAG_URL should point to your external instance
#
LIGHTRAG_URL=http://localhost:9621
LIGHTRAG_TIMEOUT=60

# LightRAG Service Configuration (when using bundled docker-compose LightRAG service)
# These settings configure the bundled LightRAG service
# If not set, sensible defaults are used

# LLM Configuration for LightRAG
LIGHTRAG_LLM_MODEL=llama3.1:8b
# LIGHTRAG_LLM_HOST=http://localhost:11434  # Defaults to LLM_BASE_URL
# LIGHTRAG_LLM_API_KEY=
# LIGHTRAG_LLM_NUM_CTX=64000

# Embedding Configuration for LightRAG
# LIGHTRAG_EMBEDDING_MODEL=nomic-embed-text
# LIGHTRAG_EMBEDDING_HOST=http://localhost:11434  # Defaults to LLM_EMBEDDING_BASE_URL or LLM_BASE_URL
# LIGHTRAG_EMBEDDING_API_KEY=
# LIGHTRAG_EMBEDDING_DIM=1536
# LIGHTRAG_EMBEDDING_NUM_CTX=4096

# CORS Configuration for LightRAG
# LIGHTRAG_CORS_ORIGINS=http://localhost:3003

# Logging Configuration for LightRAG
# LIGHTRAG_LOG_LEVEL=INFO
# LIGHTRAG_VERBOSE=False

# RAG Configuration for LightRAG
# LIGHTRAG_HISTORY_TURNS=3
# LIGHTRAG_COSINE_THRESHOLD=0.2
# LIGHTRAG_TOP_K=60
# LIGHTRAG_MAX_TOKEN_TEXT_CHUNK=4000
# LIGHTRAG_MAX_TOKEN_RELATION_DESC=4000
# LIGHTRAG_MAX_TOKEN_ENTITY_DESC=4000

# Summary Configuration for LightRAG
# LIGHTRAG_SUMMARY_LANGUAGE=English
# LIGHTRAG_MAX_TOKEN_SUMMARY=500

# Document Processing for LightRAG
# LIGHTRAG_CHUNK_SIZE=1200
# LIGHTRAG_CHUNK_OVERLAP_SIZE=100

# Application Configuration
LOG_LEVEL=INFO
LOG_FORMAT=json
DEBUG=false

# LAN Discovery Configuration
# Enable LAN discovery to allow access from other machines on the network
# When enabled, the backend will return the HOST_IP in the API config endpoint
# and CORS will allow all origins
ENABLE_LAN_DISCOVERY=false

# Host IP address for LAN discovery (e.g., 192.168.0.53)
# This should be the IP address of the machine running the backend
# Used when ENABLE_LAN_DISCOVERY=true
# HOST_IP=192.168.0.53

# Frontend API URL (optional override for development)
# If not set, defaults to http://localhost:8000
# When LAN discovery is enabled, this will be dynamically fetched from the backend
# NEXT_PUBLIC_API_URL=http://localhost:8000
