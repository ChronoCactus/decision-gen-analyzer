version: '3.8'

services:
  redis:
    image: redis:7-alpine
    ports:
      - "6380:6379"
    volumes:
      - redis_data:/data
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 3s
      retries: 5

  lightrag:
    image: ghcr.io/hkuds/lightrag:v1.4.9.3
    ports:
      - "9621:9621"
    environment:
      # Server Configuration
      - PORT=9621
      - HOST=0.0.0.0
      - WEBUI_TITLE=Decision Analyzer Graph KB
      - WEBUI_DESCRIPTION=Simple and Fast Graph Based RAG System
      - OLLAMA_EMULATING_MODEL_TAG=latest
      # LLM Configuration
      - LLM_BINDING=ollama
      - LLM_MODEL=${LIGHTRAG_LLM_MODEL:-llama3.1:8b}
      - LLM_BINDING_HOST=${LIGHTRAG_LLM_HOST:-${LLM_BASE_URL}}
      - LLM_BINDING_API_KEY=${LIGHTRAG_LLM_API_KEY:-}
      - OLLAMA_LLM_NUM_CTX=${LIGHTRAG_LLM_NUM_CTX:-64000}
      # Embedding Configuration
      - EMBEDDING_BINDING=ollama
      - EMBEDDING_BINDING_HOST=${LIGHTRAG_EMBEDDING_HOST:-${LLM_EMBEDDING_BASE_URL:-${LLM_BASE_URL}}}
      - EMBEDDING_MODEL=${LIGHTRAG_EMBEDDING_MODEL:-nomic-embed-text}
      - EMBEDDING_DIM=${LIGHTRAG_EMBEDDING_DIM:-768}
      - EMBEDDING_BINDING_API_KEY=${LIGHTRAG_EMBEDDING_API_KEY:-}
      - OLLAMA_EMBEDDING_NUM_CTX=${LIGHTRAG_EMBEDDING_NUM_CTX:-4096}
      # CORS Configuration
      - CORS_ORIGINS=${LIGHTRAG_CORS_ORIGINS:-http://localhost:3003}
      # Directory Configuration
      - INPUT_DIR=/app/data/inputs
      - WORKING_DIR=/app/data/rag_storage
      # Logging Configuration
      - LOG_LEVEL=${LIGHTRAG_LOG_LEVEL:-INFO}
      - VERBOSE=${LIGHTRAG_VERBOSE:-False}
      # RAG Configuration
      - HISTORY_TURNS=${LIGHTRAG_HISTORY_TURNS:-3}
      - COSINE_THRESHOLD=${LIGHTRAG_COSINE_THRESHOLD:-0.2}
      - TOP_K=${LIGHTRAG_TOP_K:-60}
      - MAX_TOKEN_TEXT_CHUNK=${LIGHTRAG_MAX_TOKEN_TEXT_CHUNK:-4000}
      - MAX_TOKEN_RELATION_DESC=${LIGHTRAG_MAX_TOKEN_RELATION_DESC:-4000}
      - MAX_TOKEN_ENTITY_DESC=${LIGHTRAG_MAX_TOKEN_ENTITY_DESC:-4000}
      # Summary Configuration
      - SUMMARY_LANGUAGE=${LIGHTRAG_SUMMARY_LANGUAGE:-English}
      - MAX_TOKEN_SUMMARY=${LIGHTRAG_MAX_TOKEN_SUMMARY:-500}
      # Document Processing
      - CHUNK_SIZE=${LIGHTRAG_CHUNK_SIZE:-1200}
      - CHUNK_OVERLAP_SIZE=${LIGHTRAG_CHUNK_OVERLAP_SIZE:-100}
    volumes:
      - lightrag_data:/app/data
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9621/health"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 10s
    profiles:
      - lightrag

  backend:
    build:
      context: .
      dockerfile: Dockerfile.backend
    ports:
      - "8000:8000"
    restart: unless-stopped
    environment:
      - REDIS_URL=redis://redis:6379/0
      - PYTHONPATH=/app/src
      - ADR_STORAGE_PATH=/app/data/adrs
      - LIGHTRAG_URL=${LIGHTRAG_URL:-http://lightrag:9621}
      - LIGHTRAG_API_KEY=${LIGHTRAG_API_KEY:-}
      - ENABLE_LAN_DISCOVERY=${ENABLE_LAN_DISCOVERY:-false}
      - HOST_IP=${HOST_IP:-}
      - LLM_PROVIDER=${LLM_PROVIDER:-ollama}
      - LLM_BASE_URL=${LLM_BASE_URL:-http://localhost:11434}
      - LLM_BASE_URL_1=${LLM_BASE_URL_1:-}
      - LLM_EMBEDDING_BASE_URL=${LLM_EMBEDDING_BASE_URL:-}
      - LLM_MODEL=${LLM_MODEL:-gpt-oss:20b}
      - LLM_TEMPERATURE=${LLM_TEMPERATURE:-0.7}
      - LLM_TIMEOUT=${LLM_TIMEOUT:-60}
      - OLLAMA_NUM_CTX=${OLLAMA_NUM_CTX:-64000}
      - OLLAMA_NUM_PREDICT=${OLLAMA_NUM_PREDICT:-4096}
      - OPENROUTER_API_KEY=${OPENROUTER_API_KEY:-}
    depends_on:
      redis:
        condition: service_healthy
    volumes:
      - ./src:/app/src
      - ./config:/app/config
      - ./pyproject.toml:/app/pyproject.toml
      - ./requirements.md:/app/requirements.md
      - adr_data:/app/data
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3

  celery_worker:
    build:
      context: .
      dockerfile: Dockerfile.backend
    command: celery -A src.celery_app worker --loglevel=info
    restart: unless-stopped
    environment:
      - REDIS_URL=redis://redis:6379/0
      - PYTHONPATH=/app/src
      - ADR_STORAGE_PATH=/app/data/adrs
      - LIGHTRAG_URL=${LIGHTRAG_URL:-http://lightrag:9621}
      - LIGHTRAG_API_KEY=${LIGHTRAG_API_KEY:-}
      - LLM_PROVIDER=${LLM_PROVIDER:-ollama}
      - LLM_BASE_URL=${LLM_BASE_URL:-http://localhost:11434}
      - LLM_BASE_URL_1=${LLM_BASE_URL_1:-}
      - LLM_EMBEDDING_BASE_URL=${LLM_EMBEDDING_BASE_URL:-}
      - LLM_MODEL=${LLM_MODEL:-gpt-oss:20b}
      - LLM_TEMPERATURE=${LLM_TEMPERATURE:-0.7}
      - LLM_TIMEOUT=${LLM_TIMEOUT:-60}
      - OLLAMA_NUM_CTX=${OLLAMA_NUM_CTX:-64000}
      - OLLAMA_NUM_PREDICT=${OLLAMA_NUM_PREDICT:-4096}
      - OPENROUTER_API_KEY=${OPENROUTER_API_KEY:-}
    depends_on:
      - redis
      - backend
    volumes:
      - ./src:/app/src
      - ./config:/app/config
      - ./pyproject.toml:/app/pyproject.toml
      - ./requirements.md:/app/requirements.md
      - adr_data:/app/data

  frontend:
    build:
      context: .
      dockerfile: Dockerfile.frontend
    ports:
      - "3003:3000"
    depends_on:
      - backend

volumes:
  redis_data:
  adr_data:
  lightrag_data:
