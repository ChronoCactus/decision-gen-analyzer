"""Client for interacting with llama-cpp server."""

import asyncio
from typing import Any, Dict, List, Optional
import time
from enum import Enum

import httpx

from config import get_settings
from logger import get_logger

logger = get_logger(__name__)

DEFAULT_MODEL = "gpt-oss:20b"


class ClientType(Enum):
    """Type of llama.cpp client for different purposes."""

    PRIMARY = "primary"
    SECONDARY = "secondary"
    EMBEDDING = "embedding"


class LlamaCppClient:
    """Client for llama-cpp server interactions."""

    def __init__(
        self,
        base_url: Optional[str] = None,
        timeout: Optional[int] = None,
        max_retries: int = 3,
        retry_delay: float = 1.0,
        backoff_factor: float = 2.0,
        demo_mode: bool = True,  # Enable demo mode by default
    ):
        """Initialize the llama-cpp client."""
        settings = get_settings()
        self.base_url = base_url or settings.llama_cpp_url
        self.timeout = timeout or settings.llama_cpp_timeout
        self.max_retries = max_retries
        self.retry_delay = retry_delay
        self.backoff_factor = backoff_factor
        self.demo_mode = demo_mode
        self._client: Optional[httpx.AsyncClient] = None

    async def __aenter__(self):
        """Async context manager entry."""
        self._client = httpx.AsyncClient(
            base_url=self.base_url,
            timeout=self.timeout,
        )
        return self

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """Async context manager exit."""
        if self._client:
            await self._client.aclose()

    async def generate(
        self,
        prompt: str,
        model: str = DEFAULT_MODEL,
        temperature: float = 0.7,
        num_ctx: int = 64000,
        num_predict: Optional[int] = None,
        stop: Optional[List[str]] = None,
        format: Optional[str] = None,
        **kwargs,
    ) -> str:
        """Generate text using the Ollama server or demo mode."""
        # Demo mode: simulate LLM response
        if self.demo_mode:
            logger.info("Using demo mode for LLM generation")
            await asyncio.sleep(1.5)  # Simulate processing time

            # Generate a realistic mock response based on the prompt
            if "ADR" in prompt or "decision" in prompt.lower():
                return f"""Based on the prompt "{prompt[:50]}...", I recommend the following architectural decision:

**Decision:** Adopt a microservices architecture with API Gateway pattern.

**Rationale:** This approach provides better scalability, maintainability, and allows independent deployment of services.

**Alternatives Considered:**
- Monolithic architecture (simpler but less scalable)
- Serverless functions (good for event-driven workloads)

**Trade-offs:** Increased complexity vs. better scalability and maintainability."""
            else:
                return f"""This is a simulated LLM response to: "{prompt[:100]}..."

In a real implementation, this would be generated by a large language model like Llama or GPT. The response would be contextually appropriate and based on the actual prompt provided."""

        if not self._client:
            raise RuntimeError("Client not initialized. Use as async context manager.")

        # Build options object for Ollama API
        options = {
            "temperature": temperature,
            "num_ctx": num_ctx,
        }

        if num_predict:
            options["num_predict"] = num_predict
        if stop:
            options["stop"] = stop
        if format:
            options["format"] = format

        # Add any additional options
        options.update(kwargs)

        payload = {
            "model": model,
            "prompt": prompt,
            "stream": False,
            "options": options,
        }

        if format:
            payload["format"] = format

        last_exception = None
        for attempt in range(self.max_retries + 1):
            try:
                logger.info(
                    "Sending generation request to Ollama",
                    model=model,
                    num_ctx=num_ctx,
                    attempt=attempt + 1,
                    max_attempts=self.max_retries + 1
                )

                response = await self._client.post("/api/generate", json=payload)
                response.raise_for_status()

                result = response.json()
                generated_text = result.get("response", "")

                # Validate response
                if not generated_text.strip():
                    raise ValueError("Empty response from Ollama server")

                logger.info(
                    "Generation completed successfully",
                    response_length=len(generated_text),
                    attempt=attempt + 1
                )
                return generated_text

            except httpx.TimeoutException as e:
                last_exception = e
                logger.warning(
                    "Timeout during generation attempt",
                    attempt=attempt + 1,
                    error=str(e)
                )
                if attempt < self.max_retries:
                    delay = self.retry_delay * (self.backoff_factor ** attempt)
                    logger.info(f"Retrying in {delay:.1f} seconds...")
                    await asyncio.sleep(delay)

            except httpx.HTTPError as e:
                last_exception = e
                status_code = (
                    e.response.status_code
                    if hasattr(e, "response") and e.response
                    else None
                )
                logger.warning(
                    "HTTP error during generation attempt",
                    attempt=attempt + 1,
                    status_code=status_code,
                    error=str(e)
                )
                # Don't retry on client errors (4xx), but do retry on server errors (5xx)
                if status_code and 400 <= status_code < 500:
                    break
                if attempt < self.max_retries:
                    delay = self.retry_delay * (self.backoff_factor ** attempt)
                    logger.info(f"Retrying in {delay:.1f} seconds...")
                    await asyncio.sleep(delay)

            except Exception as e:
                last_exception = e
                logger.warning(
                    "Unexpected error during generation attempt",
                    attempt=attempt + 1,
                    error=str(e)
                )
                if attempt < self.max_retries:
                    delay = self.retry_delay * (self.backoff_factor ** attempt)
                    logger.info(f"Retrying in {delay:.1f} seconds...")
                    await asyncio.sleep(delay)

        # All retries exhausted
        logger.error(
            "All generation attempts failed",
            total_attempts=self.max_retries + 1,
            final_error=str(last_exception)
        )
        raise last_exception or RuntimeError("Generation failed after all retries")

    async def list_models(self) -> List[Dict[str, Any]]:
        """List available models on the server."""
        if not self._client:
            raise RuntimeError("Client not initialized. Use as async context manager.")

        try:
            response = await self._client.get("/v1/models")
            response.raise_for_status()
            return response.json().get("data", [])
        except httpx.HTTPError as e:
            error_details = {
                "error_type": type(e).__name__,
                "error_message": str(e),
            }
            if hasattr(e, "response") and e.response is not None:
                error_details["status_code"] = e.response.status_code
                error_details["response_text"] = e.response.text[:500]
            logger.error("Failed to list models", **error_details)
            raise

    async def health_check(self) -> bool:
        """Check if the llama-cpp server is healthy."""
        if not self._client:
            raise RuntimeError("Client not initialized. Use as async context manager.")

        try:
            response = await self._client.get("/health")
            return response.status_code == 200
        except httpx.HTTPError:
            return False


class LlamaCppClientPool:
    """Pool of llama-cpp clients for parallel request processing."""

    def __init__(
        self,
        timeout: Optional[int] = None,
        max_retries: int = 3,
        retry_delay: float = 1.0,
        backoff_factor: float = 2.0,
        demo_mode: bool = True,
    ):
        """Initialize the client pool with URLs from settings."""
        settings = get_settings()
        self.timeout = timeout or settings.llama_cpp_timeout
        self.max_retries = max_retries
        self.retry_delay = retry_delay
        self.backoff_factor = backoff_factor
        self.demo_mode = demo_mode

        # Build list of available backend URLs
        self.generation_urls = [settings.llama_cpp_url]
        if settings.llama_cpp_url_1:
            self.generation_urls.append(settings.llama_cpp_url_1)

        self.embedding_url = settings.llama_cpp_url_embedding or settings.llama_cpp_url

        logger.info(
            "Initialized LlamaCppClientPool",
            generation_backends=len(self.generation_urls),
            generation_urls=self.generation_urls,
            embedding_url=self.embedding_url,
        )

        self._clients: Dict[str, LlamaCppClient] = {}

    async def __aenter__(self):
        """Async context manager entry - initialize all clients."""
        # Create clients for each generation URL
        for idx, url in enumerate(self.generation_urls):
            client = LlamaCppClient(
                base_url=url,
                timeout=self.timeout,
                max_retries=self.max_retries,
                retry_delay=self.retry_delay,
                backoff_factor=self.backoff_factor,
                demo_mode=self.demo_mode,
            )
            await client.__aenter__()
            self._clients[f"gen_{idx}"] = client

        # Create dedicated embedding client if different from primary
        if self.embedding_url != self.generation_urls[0]:
            embedding_client = LlamaCppClient(
                base_url=self.embedding_url,
                timeout=self.timeout,
                max_retries=self.max_retries,
                retry_delay=self.retry_delay,
                backoff_factor=self.backoff_factor,
                demo_mode=self.demo_mode,
            )
            await embedding_client.__aenter__()
            self._clients["embedding"] = embedding_client

        logger.info("All clients initialized in pool", client_count=len(self._clients))
        return self

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """Async context manager exit - close all clients."""
        for client in self._clients.values():
            await client.__aexit__(exc_type, exc_val, exc_tb)
        self._clients.clear()

    def get_generation_client(self, index: int = 0) -> LlamaCppClient:
        """Get a generation client by index (round-robin for parallel requests).

        Args:
            index: Index to select which generation backend to use

        Returns:
            LlamaCppClient for generation requests
        """
        client_idx = index % len(self.generation_urls)
        client_key = f"gen_{client_idx}"
        if client_key not in self._clients:
            raise RuntimeError(
                "Client pool not initialized. Use as async context manager."
            )
        return self._clients[client_key]

    def get_embedding_client(self) -> LlamaCppClient:
        """Get the dedicated embedding client, or fallback to primary.

        Returns:
            LlamaCppClient for embedding requests
        """
        if "embedding" in self._clients:
            return self._clients["embedding"]
        # Fallback to primary generation client
        return self.get_generation_client(0)

    async def generate_parallel(
        self,
        prompts: List[str],
        model: str = DEFAULT_MODEL,
        temperature: float = 0.7,
        **kwargs,
    ) -> List[str]:
        """Generate responses for multiple prompts in parallel.

        Args:
            prompts: List of prompts to process
            model: Model name to use
            temperature: Temperature for generation
            **kwargs: Additional generation parameters

        Returns:
            List of generated responses in same order as prompts
        """
        if not prompts:
            return []

        logger.info(
            "Starting parallel generation",
            prompt_count=len(prompts),
            backend_count=len(self.generation_urls),
        )

        # Create tasks for each prompt, distributing across available clients
        tasks = []
        for idx, prompt in enumerate(prompts):
            client = self.get_generation_client(idx)
            task = client.generate(
                prompt=prompt, model=model, temperature=temperature, **kwargs
            )
            tasks.append(task)

        # Execute all tasks in parallel
        results = await asyncio.gather(*tasks, return_exceptions=True)

        # Check for exceptions and log them
        for idx, result in enumerate(results):
            if isinstance(result, Exception):
                logger.error(
                    "Parallel generation task failed", task_index=idx, error=str(result)
                )

        # Return results, converting exceptions to empty strings
        return [r if isinstance(r, str) else "" for r in results]
