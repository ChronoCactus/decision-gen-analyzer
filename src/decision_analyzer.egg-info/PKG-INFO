Metadata-Version: 2.4
Name: decision-analyzer
Version: 0.1.0
Summary: AI-powered decision record analyzer using multiple personas
Author: Decision Analyzer Team
Requires-Python: >=3.9
Description-Content-Type: text/markdown
License-File: LICENSE
Requires-Dist: httpx>=0.25.0
Requires-Dist: pydantic>=2.0.0
Requires-Dist: pydantic-settings>=2.0.0
Requires-Dist: python-dotenv>=1.0.0
Requires-Dist: structlog>=23.0.0
Requires-Dist: pytest>=7.0.0
Requires-Dist: pytest-asyncio>=0.21.0
Requires-Dist: pytest-cov>=4.1.0
Requires-Dist: pytest-mock>=3.12.0
Requires-Dist: lightrag-hku>=0.1.0
Requires-Dist: pyyaml>=6.0.0
Requires-Dist: fastapi>=0.104.0
Requires-Dist: uvicorn[standard]>=0.24.0
Requires-Dist: redis>=5.0.0
Requires-Dist: celery>=5.3.0
Requires-Dist: aiofiles>=23.0.0
Requires-Dist: python-multipart>=0.0.6
Requires-Dist: langchain-openai>=0.2.0
Requires-Dist: langchain-core>=0.3.0
Provides-Extra: dev
Requires-Dist: black>=23.0.0; extra == "dev"
Requires-Dist: isort>=5.12.0; extra == "dev"
Requires-Dist: mypy>=1.0.0; extra == "dev"
Requires-Dist: ruff>=0.1.0; extra == "dev"
Dynamic: license-file

# Decision Analyzer

AI-powered system for analyzing Architectural Decision Records (ADRs) using multiple personas and vector-based retrieval.

## Features

- **Multi-Persona Analysis**: Analyze ADRs from different viewpoints (engineer, customer support, philosopher, etc.)
- **Vector Storage**: Store and retrieve ADRs using LightRAG for semantic search
- **LLM Integration**: Connect to llama-cpp servers for AI-powered analysis
- **ADR Generation**: Create new ADRs based on context and related decisions
- **Conflict Detection**: Identify conflicts and continuity issues across ADRs
- **Periodic Re-analysis**: Automated web search and re-assessment of decisions
- **Web UI**: Modern React interface for ADR management and analysis
- **Queue System**: Async processing with Redis and Celery for bulk operations

## Quick Start with Docker

1. Clone the repository:
```bash
git clone <repository-url>
cd decision-analyzer
```

2. Configure your environment (copy `.env.example` to `.env` and update):
```bash
cp .env.example .env
# Edit .env to set LLAMA_CPP_URL to your Ollama server
```

3. Start all services with bundled LightRAG:
```bash
docker compose --profile lightrag up --build
```

Or without bundled LightRAG (requires external LightRAG instance):
```bash
docker compose up --build
```

4. Open your browser to `http://localhost:3003`

For detailed configuration options and troubleshooting, see [docs/QUICKSTART.md](docs/QUICKSTART.md).

The system includes:
- **Frontend**: React/Next.js UI on port 3003
- **Backend API**: FastAPI server on port 8000
- **Redis**: Queue system on port 6379
- **Celery Worker**: Background task processing
- **LightRAG**: Vector database and RAG service on port 9621 (optional via `--profile lightrag`)

> **Note**: The system includes an optional bundled LightRAG service. Use `--profile lightrag` to enable it, or point to your own external LightRAG instance. See [LightRAG Configuration](#lightrag-configuration) for details.

## Development Setup

### Backend Setup

1. Install Python dependencies:
```bash
pip install -e .
```

2. Start Redis (if not using Docker):
```bash
redis-server
```

3. Run the backend API:
```bash
./scripts/run_backend.sh
```

### Frontend Setup

1. Install Node.js dependencies:
```bash
cd frontend
npm install
```

2. Start the development server:
```bash
npm run dev
```

3. Open `http://localhost:3003`

### Manual Configuration

Copy and edit environment files:
```bash
cp .env.example .env
# Frontend .env.local can be created manually if needed for local overrides
```

## API Documentation

When running, visit `http://localhost:8000/docs` for interactive API documentation.

### Key Endpoints

- `GET /api/v1/adrs/` - List all ADRs
- `POST /api/v1/analysis/analyze` - Queue ADR analysis
- `POST /api/v1/generation/generate` - Queue ADR generation
- `GET /api/v1/analysis/task/{task_id}` - Check analysis task status
- `GET /api/v1/generation/task/{task_id}` - Check generation task status

## Architecture

```mermaid
graph TD
    UI[React UI<br/>Port 3003]
    API[FastAPI Backend<br/>Port 8000]
    Celery[Celery Workers]
    Redis[Redis Queue<br/>Port 6379]
    Llama[Llama.cpp Server<br/>Port 11434<br/>External]
    LightRAG[LightRAG Vector DB<br/>Port 9621<br/>Optional]
    
    UI -->|HTTP Requests| API
    API -->|Queue Tasks| Redis
    API -->|Direct Calls| LightRAG
    Celery -->|Poll Tasks| Redis
    Celery -->|LLM Generation| Llama
    Celery -->|RAG Retrieval| LightRAG
    LightRAG -->|Embeddings & LLM| Llama
```

## Usage

### Web Interface

1. **View ADRs**: Browse all ADRs in a card-based layout
2. **Analyze ADR**: Click "Analyze" to queue AI analysis with multiple personas
3. **Generate ADR**: Use the "Generate New ADR" button to create ADRs from prompts
4. **Track Progress**: Monitor queued tasks with real-time status updates

### Programmatic Usage

```python
from decision_analyzer import setup_logging, get_settings, LlamaCppClient, LightRAGClient

# Setup logging
setup_logging()

# Get configuration
settings = get_settings()

# Use clients
async with LlamaCppClient() as llama:
    response = await llama.generate("Analyze this decision...")

async with LightRAGClient() as lightrag:
    await lightrag.store_document("adr-001", "ADR content here")
```

## Testing

### Quick Start

Run all tests (backend + frontend):
```bash
make test
```

### Backend Tests

```bash
make test-backend              # All backend tests
make test-backend-unit         # Unit tests only
make test-backend-integration  # Integration tests only
make test-coverage-backend     # With coverage report
```

### Frontend Tests

First, install frontend test dependencies:
```bash
make install-frontend-deps
```

Then run tests:
```bash
make test-frontend             # Run all frontend tests
make test-coverage-frontend    # With coverage report
```

### Test Structure

**Backend**: Tests are co-located with source code in `tests/` subdirectories
```
src/adr_generation.py → src/tests/test_adr_generation.py
```

**Frontend**: Tests live alongside source files with `.test.tsx` extension
```
components/ADRCard.tsx → components/ADRCard.test.tsx
```

**Integration Tests**: Located in `tests/integration/`

See [docs/TESTING.md](docs/TESTING.md) for comprehensive testing guidelines.

## Development

### Code Quality

```bash
# Format code
black src/ tests/

# Sort imports
isort src/ tests/

# Type checking
mypy src/

# Linting
ruff check src/ tests/
```

## Project Structure

```
decision-analyzer/
├── src/
│   ├── __init__.py
│   ├── config.py          # Configuration management
│   ├── logger.py          # Logging setup
│   ├── models.py          # Data models (ADR, AnalysisResult, etc.)
│   ├── llama_client.py    # llama-cpp server client
│   └── lightrag_client.py # LightRAG server client
├── tests/
│   └── test_infrastructure.py
├── docs/
├── pyproject.toml
├── README.md
└── .env.example
```

## Configuration

The application uses the following environment variables:

### LLM Configuration
- `LLAMA_CPP_URL`: Primary URL of the llama-cpp server (required, default: http://localhost:11434)
- `LLAMA_CPP_URL_1`: Secondary llama-cpp server for parallel processing (optional)
- `LLAMA_CPP_URL_EMBEDDING`: Dedicated server for embeddings (optional)

See [docs/PARALLEL_PROCESSING.md](docs/PARALLEL_PROCESSING.md) for details on multi-backend configuration.

### LAN Discovery Configuration
Enable access from other machines on your local network:

- `ENABLE_LAN_DISCOVERY`: Set to `true` to allow access from other machines (default: false)
- `HOST_IP`: Your machine's IP address (e.g., `192.168.0.53`)

**Example for LAN access:**
```bash
# In your .env file or docker-compose environment
ENABLE_LAN_DISCOVERY=true
HOST_IP=192.168.0.53
```

When enabled:
- Frontend dynamically discovers the backend URL from the API
- CORS allows all origins (instead of just localhost)
- Access the UI from any device: `http://192.168.0.53:3003`
- The frontend will automatically use `http://192.168.0.53:8000` for API calls

See [docs/LAN_DISCOVERY.md](docs/LAN_DISCOVERY.md) for complete setup guide and troubleshooting.

**Security Note**: Only enable LAN discovery on trusted networks. This feature opens your backend to all devices on the network.

### LightRAG Configuration

The system includes an **optional bundled LightRAG service** that provides vector storage and graph-based RAG capabilities. No separate LightRAG deployment is required!

**Using the Bundled Service**

Start with the `lightrag` profile:
```bash
docker compose --profile lightrag up --build
```

The LightRAG service will automatically:
- Use your existing Ollama server (configured via `LLAMA_CPP_URL`)
- Start on port 9621
- Persist data in a Docker volume

No additional configuration needed! Just make sure your Ollama server is accessible.

**Customizing the Bundled Service**

Override defaults in your `.env`:
```bash
# Use different models
LIGHTRAG_LLM_MODEL=llama3.1:8b
LIGHTRAG_EMBEDDING_MODEL=nomic-embed-text

# Tune RAG performance
LIGHTRAG_TOP_K=100
LIGHTRAG_COSINE_THRESHOLD=0.3

# Use dedicated Ollama servers for LightRAG
LIGHTRAG_LLM_HOST=http://192.168.0.200:11434
LIGHTRAG_EMBEDDING_HOST=http://192.168.0.201:11434
```

**Using an External LightRAG Instance**

If you have an existing LightRAG deployment:
```bash
# In .env - point to your instance
LIGHTRAG_URL=http://your-lightrag-server:9621
LIGHTRAG_API_KEY=your-api-key

# Start without the bundled service (no --profile flag)
docker compose up --build
```

### Other Configuration
- `LOG_LEVEL`: Logging level (default: INFO)
- `LOG_FORMAT`: Log format, either 'json' or 'text' (default: json)

## Contributing

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Add tests
5. Run the test suite
6. Submit a pull request

## License

Apache 2.0 License. See [LICENSE](LICENSE) for details.
